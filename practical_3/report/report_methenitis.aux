\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Task 1}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $128$ and no regularization. Green lines illustrate performance on the training set and blue lines on the test set.}}{1}{figure.1}}
\newlabel{fig:1}{{1}{1}{Accuracy (left) and loss (right) for the CNN with batch size of $128$ and no regularization. Green lines illustrate performance on the training set and blue lines on the test set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Regularization}{2}{subsection.1.1}}
\newlabel{sec:reg}{{1.1}{2}{Regularization}{subsection.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $128$ and no regularization. Cyan lines illustrate performance on the training set and orange lines on the test set.}}{2}{figure.2}}
\newlabel{fig:2}{{2}{2}{Accuracy (left) and loss (right) for the CNN with batch size of $128$ and no regularization. Cyan lines illustrate performance on the training set and orange lines on the test set}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Dropout}{2}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $128$ and dropout. Red lines illustrate performance on the training set and purple lines on the test set for dropout $0.2$, while light green lines illustrate performance on the training set and yellow lines on the test set for dropout $0.5$.}}{2}{figure.3}}
\newlabel{fig:3}{{3}{2}{Accuracy (left) and loss (right) for the CNN with batch size of $128$ and dropout. Red lines illustrate performance on the training set and purple lines on the test set for dropout $0.2$, while light green lines illustrate performance on the training set and yellow lines on the test set for dropout $0.5$}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Smaller Batch Size}{3}{subsection.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set for dropout $0.2$. Green and blue lines illustrate the performance of batch size $128$ with default configuration CNN as in all previous figures.}}{3}{figure.4}}
\newlabel{fig:4}{{4}{3}{Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set for dropout $0.2$. Green and blue lines illustrate the performance of batch size $128$ with default configuration CNN as in all previous figures}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Regularization}{3}{subsubsection.1.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set without weight regularization. Red lines illustrate performance on the training set and purple lines on the test set with weight regularization $0.01$. Light purple lines illustrate performance on the training set and cyan lines on the test set with weight regularization $0.005$.}}{3}{figure.5}}
\newlabel{fig:5}{{5}{3}{Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set without weight regularization. Red lines illustrate performance on the training set and purple lines on the test set with weight regularization $0.01$. Light purple lines illustrate performance on the training set and cyan lines on the test set with weight regularization $0.005$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Dropout}{4}{subsubsection.1.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set without weight regularization. Red lines illustrate performance on the training set and purple lines on the test set with weight regularization $0.01$. Light purple lines illustrate performance on the training set and cyan lines on the test set with weight regularization $0.005$.}}{4}{figure.6}}
\newlabel{fig:6}{{6}{4}{Accuracy (left) and loss (right) for the CNN with batch size of $64$. Cyan lines illustrate performance on the training set and orange lines on the test set without weight regularization. Red lines illustrate performance on the training set and purple lines on the test set with weight regularization $0.01$. Light purple lines illustrate performance on the training set and cyan lines on the test set with weight regularization $0.005$}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Feature Visualization}{4}{subsection.1.4}}
\newlabel{sec:vis}{{1.4}{4}{Feature Visualization}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}One-vs-Rest Classifier}{4}{subsubsection.1.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of the features in the 2-d space of the \texttt  {flatten} layer (top), \texttt  {fc1} (bottom-left), \texttt  {fc2} (bottom-right).}}{5}{figure.7}}
\newlabel{fig:7}{{7}{5}{Visualization of the features in the 2-d space of the \texttt {flatten} layer (top), \texttt {fc1} (bottom-left), \texttt {fc2} (bottom-right)}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Task 2}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dataset}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Loss function}{7}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training}{7}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Experiment 1} Loss in the training (purple line) and the test set (yellow line) for $50000$ training steps, $margin = 1.0$, using the default \texttt  {next\_batch} method and the loss function (1).}}{7}{figure.8}}
\newlabel{fig:siamese}{{8}{7}{\textbf {Experiment 1} Loss in the training (purple line) and the test set (yellow line) for $50000$ training steps, $margin = 1.0$, using the default \texttt {next\_batch} method and the loss function (1)}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Experiment 2} (Different Loss Functions) Loss in the training (blue line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt  {next\_batch} method and the loss function (2). Loss in the training (purple line) and the test set (yellow line) for $50000$ training steps, $margin = 1.0$, using the default \texttt  {next\_batch} method and the loss function (1).}}{8}{figure.9}}
\newlabel{fig:siamese-1}{{9}{8}{\textbf {Experiment 2} (Different Loss Functions) Loss in the training (blue line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt {next\_batch} method and the loss function (2). Loss in the training (purple line) and the test set (yellow line) for $50000$ training steps, $margin = 1.0$, using the default \texttt {next\_batch} method and the loss function (1)}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Experiment 3} (Default vs Alternative Batch Creation) Loss in the training (blue line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt  {next\_batch} method and the loss function (2). Loss in the training (yellow line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt  {next\_batch} method and the loss function (2).}}{8}{figure.10}}
\newlabel{fig:siamese-2}{{10}{8}{\textbf {Experiment 3} (Default vs Alternative Batch Creation) Loss in the training (blue line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt {next\_batch} method and the loss function (2). Loss in the training (yellow line) and the test set (light purple line) for $50000$ training steps, $margin = 1.0$, using the default \texttt {next\_batch} method and the loss function (2)}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feature Visualization}{8}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}One-vs-Rest Classifier}{8}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Visualization of the $2D$ features generated out of the whole test set. Experiment 1 (top left), experiment 2 (top right), experiment 3 (bottom).}}{9}{figure.11}}
\newlabel{fig:features}{{11}{9}{Visualization of the $2D$ features generated out of the whole test set. Experiment 1 (top left), experiment 2 (top right), experiment 3 (bottom)}{figure.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Task 3: Transfer Learning}{10}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph for the task 3 of the assignment, the final three layers of convnet were used. Note the stop gradient operation, where we don't want to update the pre-trained weights.}}{10}{figure.12}}
\newlabel{fig:graph}{{12}{10}{Graph for the task 3 of the assignment, the final three layers of convnet were used. Note the stop gradient operation, where we don't want to update the pre-trained weights}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feature Extraction}{10}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Retraining}{10}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Refining}{10}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Performance of transfer learning on Cifar10}{10}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces }}{11}{figure.13}}
\newlabel{fig:extra}{{13}{11}{}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces }}{12}{figure.14}}
\newlabel{fig:retrain}{{14}{12}{}{figure.14}{}}
